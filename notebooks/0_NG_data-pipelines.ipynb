{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data pipeline has the objective to download and transform the original dataset to obtain our final dataset. Moreover, we will use DVC to keep data versioning control in every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Necessary imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import subprocess\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(sample_list, src_images_dir, src_masks_dir, dest_images_dir, dest_masks_dir):\n",
    "    for sample in sample_list:\n",
    "        # Copy image file\n",
    "        src_image_path = os.path.join(src_images_dir, sample)\n",
    "        dest_image_path = os.path.join(dest_images_dir, sample)\n",
    "        shutil.copyfile(src_image_path, dest_image_path)\n",
    "\n",
    "        # Copy mask file (assuming the mask file has the same name as the image file)\n",
    "        sample_mask = sample.replace('jpg', 'png')\n",
    "\n",
    "        src_mask_path = os.path.join(src_masks_dir, sample_mask)\n",
    "        dest_mask_path = os.path.join(dest_masks_dir, sample_mask)\n",
    "        shutil.copyfile(src_mask_path, dest_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_raw_masks_to_image_masks(input_dirs: list[str], output_dirs: list[str]) -> None:\n",
    "    for input_dir, output_dir in zip(input_dirs, output_dirs):\n",
    "        # Process each directories masks\n",
    "        palette: list[int] = [\n",
    "                0, 0, 0, # For background -> Black\n",
    "                255, 0, 0, # For persons -> Red\n",
    "            ]\n",
    "\n",
    "        for j in os.listdir(input_dir):\n",
    "            if j == '.DS_Store':\n",
    "                continue\n",
    "            \n",
    "            image_path = input_dir / j\n",
    "            mask = Image.open(image_path).convert('P')\n",
    "            \n",
    "            # Ensure that all non-zero values are set to 1\n",
    "            mask_data = mask.load()\n",
    "            width, height = mask.size\n",
    "            for y in range(height):\n",
    "                for x in range(width):\n",
    "                    if mask_data[x, y] > 0:\n",
    "                        mask_data[x, y] = 1\n",
    "            \n",
    "            mask.putpalette(palette)\n",
    "            save_path = output_dir / j\n",
    "            mask.save(save_path, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_image_masks_to_labels(input_dirs: list[str], output_dirs: list[str]) -> None:\n",
    "    for input_dir, output_dir in zip(input_dirs, output_dirs):\n",
    "        for j in os.listdir(input_dir):\n",
    "            if j == '.DS_Store':\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(input_dir, j)\n",
    "            # load the binary mask and get its contours\n",
    "            mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            H, W = mask.shape\n",
    "            contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # convert the contours to polygons\n",
    "            polygons = []\n",
    "            for cnt in contours:\n",
    "                if cv2.contourArea(cnt) > 200:\n",
    "                    polygon = []\n",
    "                    for point in cnt:\n",
    "                        x, y = point[0]\n",
    "                        polygon.append(x / W)\n",
    "                        polygon.append(y / H)\n",
    "                    polygons.append(polygon)\n",
    "\n",
    "            # print the polygons\n",
    "            with open('{}.txt'.format(os.path.join(output_dir, j)[:-4]), 'w') as f:\n",
    "                for polygon in polygons:\n",
    "                    for p_, p in enumerate(polygon):\n",
    "                        if p_ == len(polygon) - 1:\n",
    "                            f.write('{}\\n'.format(p))\n",
    "                        elif p_ == 0:\n",
    "                            f.write('0 {} '.format(p))\n",
    "                        else:\n",
    "                            f.write('{} '.format(p))\n",
    "\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Varibale definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-07 23:49:04.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mperson_image_segmentation.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/nachogris/Desktop/UNI/GCED/QUART/TAED2/LAB/TAED2_YOLOs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "\n",
    "from person_image_segmentation.config import DATASET_LINK, DATA_DIR, SPLIT_DATA_DIR, TRANSFORM_DATA_DIR, LABELS_DATA_DIR, TRAIN_SIZE, VAL_SIZE, TEST_SIZE, KAGGLE_KEY, KAGGLE_USERNAME\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create data directory if it does not exist\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up Kaggle credentials from environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download the original data. To do this we will use `KaggleAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mariarisques/dataset-person-yolos\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "api.dataset_download_files(DATASET_LINK, path = DATA_DIR, unzip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to split the data into train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "images_dir = DATA_DIR / 'dataset_person-yolos/data/images'\n",
    "masks_dir = DATA_DIR / 'dataset_person-yolos/data/masks'\n",
    "images_dir_train = SPLIT_DATA_DIR / 'images/train'\n",
    "masks_dir_train = SPLIT_DATA_DIR / 'masks/train'\n",
    "images_dir_val = SPLIT_DATA_DIR / 'images/val'\n",
    "masks_dir_val = SPLIT_DATA_DIR / 'masks/val'\n",
    "images_dir_test = SPLIT_DATA_DIR / 'images/test'\n",
    "masks_dir_test = SPLIT_DATA_DIR / 'masks/test'\n",
    "\n",
    "# Get the list of samples and shuffle them\n",
    "samples = os.listdir(images_dir)\n",
    "random.shuffle(samples)\n",
    "\n",
    "# Calculate split indices\n",
    "num_samples = len(samples)\n",
    "train_end = int(TRAIN_SIZE * num_samples)\n",
    "val_end = train_end + int(VAL_SIZE * num_samples)\n",
    "\n",
    "# Split samples\n",
    "train_samples = samples[:train_end]\n",
    "val_samples = samples[train_end:val_end]\n",
    "test_samples = samples[val_end:]\n",
    "\n",
    "# Create necessary directories\n",
    "SPLIT_DATA_DIR.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_samples, images_dir, masks_dir, images_dir_train, masks_dir_train)\n",
    "copy_files(val_samples, images_dir, masks_dir, images_dir_val, masks_dir_val)\n",
    "copy_files(test_samples, images_dir, masks_dir, images_dir_test, masks_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we need to go from the original masks to ones that can be later transformed to labels that yolo is able to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\marcj\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\person-image-segmentation--tCbR_TX-py3.10\\lib\\site-packages\\PIL\\ImageFile.py:547\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    548\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m output_dir_val\u001b[38;5;241m.\u001b[39mmkdir(parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m output_dir_test\u001b[38;5;241m.\u001b[39mmkdir(parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mfrom_raw_masks_to_image_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_dir_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dir_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dir_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_dir_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir_test\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m images_dir_trans_train \u001b[38;5;241m=\u001b[39m TRANSFORM_DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m images_dir_trans_val \u001b[38;5;241m=\u001b[39m TRANSFORM_DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/val\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m, in \u001b[0;36mfrom_raw_masks_to_image_masks\u001b[1;34m(input_dirs, output_dirs)\u001b[0m\n\u001b[0;32m     24\u001b[0m mask\u001b[38;5;241m.\u001b[39mputpalette(palette)\n\u001b[0;32m     25\u001b[0m save_path \u001b[38;5;241m=\u001b[39m output_dir \u001b[38;5;241m/\u001b[39m j\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPNG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marcj\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\person-image-segmentation--tCbR_TX-py3.10\\lib\\site-packages\\PIL\\Image.py:2568\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[0;32m   2567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2568\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\marcj\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\person-image-segmentation--tCbR_TX-py3.10\\lib\\site-packages\\PIL\\PngImagePlugin.py:1431\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1428\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[0;32m   1429\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\marcj\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\person-image-segmentation--tCbR_TX-py3.10\\lib\\site-packages\\PIL\\ImageFile.py:551\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    549\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 551\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    553\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\marcj\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\person-image-segmentation--tCbR_TX-py3.10\\lib\\site-packages\\PIL\\ImageFile.py:570\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 570\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    571\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dir_train = SPLIT_DATA_DIR / 'masks/train'\n",
    "output_dir_train = TRANSFORM_DATA_DIR / 'masks/train'\n",
    "input_dir_val = SPLIT_DATA_DIR / 'masks/val'\n",
    "output_dir_val = TRANSFORM_DATA_DIR / 'masks/val'\n",
    "input_dir_test = SPLIT_DATA_DIR / 'masks/test'\n",
    "output_dir_test = TRANSFORM_DATA_DIR / 'masks/test'\n",
    "\n",
    "output_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "from_raw_masks_to_image_masks(\n",
    "    input_dirs = [input_dir_train, input_dir_val, input_dir_test],\n",
    "    output_dirs = [output_dir_train, output_dir_val, output_dir_test]\n",
    ")\n",
    "\n",
    "images_dir_trans_train = TRANSFORM_DATA_DIR / 'images/train'\n",
    "images_dir_trans_val = TRANSFORM_DATA_DIR / 'images/val'\n",
    "images_dir_trans_test = TRANSFORM_DATA_DIR / 'images/test'\n",
    "\n",
    "shutil.copytree(images_dir_train, images_dir_trans_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_val, images_dir_trans_val, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_test, images_dir_trans_test, dirs_exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step we convert the previous transformed masks to some labels, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/nachogris/Desktop/UNI/GCED/QUART/TAED2/LAB/TAED2_YOLOs/data/processed/images/train')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir_train = TRANSFORM_DATA_DIR / 'masks/train'\n",
    "output_dir_train = LABELS_DATA_DIR / 'labels/train'\n",
    "input_dir_val = TRANSFORM_DATA_DIR / 'masks/val'\n",
    "output_dir_val = LABELS_DATA_DIR / 'labels/val'\n",
    "input_dir_test = TRANSFORM_DATA_DIR / 'masks/test'\n",
    "output_dir_test = LABELS_DATA_DIR / 'labels/test'\n",
    "\n",
    "output_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "from_image_masks_to_labels(\n",
    "    input_dirs = [input_dir_train, input_dir_val, input_dir_test],\n",
    "    output_dirs = [output_dir_train, output_dir_val, output_dir_test]\n",
    ")\n",
    "\n",
    "images_dir_labels_train = LABELS_DATA_DIR / 'images/train'\n",
    "images_dir_labels_val = LABELS_DATA_DIR / 'images/val'\n",
    "images_dir_labels_test = LABELS_DATA_DIR / 'images/test'\n",
    "\n",
    "shutil.copytree(images_dir_train, images_dir_labels_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_val, images_dir_labels_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_test, images_dir_labels_train, dirs_exist_ok = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "person-image-segmentation-leWn8e5q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
