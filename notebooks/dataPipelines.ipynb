{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data pipeline has the objective to download and transform the original dataset to obtain our final dataset. Moreover, we will use DVC to keep data versioning control in every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Necessary imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import subprocess\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(sample_list, src_images_dir, src_masks_dir, dest_images_dir, dest_masks_dir):\n",
    "    for sample in sample_list:\n",
    "        # Copy image file\n",
    "        src_image_path = os.path.join(src_images_dir, sample)\n",
    "        dest_image_path = os.path.join(dest_images_dir, sample)\n",
    "        shutil.copyfile(src_image_path, dest_image_path)\n",
    "\n",
    "        # Copy mask file (assuming the mask file has the same name as the image file)\n",
    "        sample_mask = sample.replace('jpg', 'png')\n",
    "\n",
    "        src_mask_path = os.path.join(src_masks_dir, sample_mask)\n",
    "        dest_mask_path = os.path.join(dest_masks_dir, sample_mask)\n",
    "        shutil.copyfile(src_mask_path, dest_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_raw_masks_to_image_masks(input_dirs: list[str], output_dirs: list[str]) -> None:\n",
    "    for input_dir, output_dir in zip(input_dirs, output_dirs):\n",
    "        # Process each directories masks\n",
    "        palette: list[int] = [\n",
    "                0, 0, 0, # For background -> Black\n",
    "                255, 0, 0, # For persons -> Red\n",
    "            ]\n",
    "\n",
    "        for j in os.listdir(input_dir):\n",
    "            if j == '.DS_Store':\n",
    "                continue\n",
    "            \n",
    "            image_path = input_dir / j\n",
    "            mask = Image.open(image_path).convert('P')\n",
    "            \n",
    "            # Ensure that all non-zero values are set to 1\n",
    "            mask_data = mask.load()\n",
    "            width, height = mask.size\n",
    "            for y in range(height):\n",
    "                for x in range(width):\n",
    "                    if mask_data[x, y] > 0:\n",
    "                        mask_data[x, y] = 1\n",
    "            \n",
    "            mask.putpalette(palette)\n",
    "            save_path = output_dir / j\n",
    "            mask.save(save_path, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_image_masks_to_labels(input_dirs: list[str], output_dirs: list[str]) -> None:\n",
    "    for input_dir, output_dir in zip(input_dirs, output_dirs):\n",
    "        for j in os.listdir(input_dir):\n",
    "            if j == '.DS_Store':\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(input_dir, j)\n",
    "            # load the binary mask and get its contours\n",
    "            mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            H, W = mask.shape\n",
    "            contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # convert the contours to polygons\n",
    "            polygons = []\n",
    "            for cnt in contours:\n",
    "                if cv2.contourArea(cnt) > 200:\n",
    "                    polygon = []\n",
    "                    for point in cnt:\n",
    "                        x, y = point[0]\n",
    "                        polygon.append(x / W)\n",
    "                        polygon.append(y / H)\n",
    "                    polygons.append(polygon)\n",
    "\n",
    "            # print the polygons\n",
    "            with open('{}.txt'.format(os.path.join(output_dir, j)[:-4]), 'w') as f:\n",
    "                for polygon in polygons:\n",
    "                    for p_, p in enumerate(polygon):\n",
    "                        if p_ == len(polygon) - 1:\n",
    "                            f.write('{}\\n'.format(p))\n",
    "                        elif p_ == 0:\n",
    "                            f.write('0 {} '.format(p))\n",
    "                        else:\n",
    "                            f.write('{} '.format(p))\n",
    "\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Varibale definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables are going to be defined based in a configuration file that will ease the process of variable modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML configuration file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Declare some configuration variables\n",
    "DATASET_LINK = config['dataPipelines']['dataDownloading']['datasetLink']\n",
    "DATA_DIR = Path(config['dataPipelines']['dataDownloading']['dataDirectory'])\n",
    "TRAIN_SIZE = config['dataPipelines']['splitData']['trainSize']\n",
    "VAL_SIZE = config['dataPipelines']['splitData']['valSize']\n",
    "TEST_SIZE = config['dataPipelines']['splitData']['testSize']\n",
    "SPLIT_DATA_DIR = Path(config['dataPipelines']['splitData']['dataDirectory'])\n",
    "TRANSFORM_DATA_DIR = Path(config['dataPipelines']['transformMasks']['dataDirectory'])\n",
    "LABELS_DATA_DIR = Path(config['dataPipelines']['createLabels']['dataDirectory'])\n",
    "\n",
    "# Create data directory if it does not exist\n",
    "DATA_DIR.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Load environment variables from a .env file and set up Kaggle credentials from environment variables\n",
    "load_dotenv()\n",
    "os.environ['KAGGLE_USERNAME'] = os.getenv('KAGGLE_USERNAME')\n",
    "os.environ['KAGGLE_KEY'] = os.getenv('KAGGLE_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download the original data. To do this we will use `KaggleAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mariarisques/dataset-person-yolos\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "api.dataset_download_files(DATASET_LINK, path = DATA_DIR, unzip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to split the data into train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "images_dir = DATA_DIR / 'dataset_person-yolos/data/images'\n",
    "masks_dir = DATA_DIR / 'dataset_person-yolos/data/masks'\n",
    "images_dir_train = SPLIT_DATA_DIR / 'images/train'\n",
    "masks_dir_train = SPLIT_DATA_DIR / 'masks/train'\n",
    "images_dir_val = SPLIT_DATA_DIR / 'images/val'\n",
    "masks_dir_val = SPLIT_DATA_DIR / 'masks/val'\n",
    "images_dir_test = SPLIT_DATA_DIR / 'images/test'\n",
    "masks_dir_test = SPLIT_DATA_DIR / 'masks/test'\n",
    "\n",
    "# Get the list of samples and shuffle them\n",
    "samples = os.listdir(images_dir)\n",
    "random.shuffle(samples)\n",
    "\n",
    "# Calculate split indices\n",
    "num_samples = len(samples)\n",
    "train_end = int(TRAIN_SIZE * num_samples)\n",
    "val_end = train_end + int(VAL_SIZE * num_samples)\n",
    "\n",
    "# Split samples\n",
    "train_samples = samples[:train_end]\n",
    "val_samples = samples[train_end:val_end]\n",
    "test_samples = samples[val_end:]\n",
    "\n",
    "# Create necessary directories\n",
    "SPLIT_DATA_DIR.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_samples, images_dir, masks_dir, images_dir_train, masks_dir_train)\n",
    "copy_files(val_samples, images_dir, masks_dir, images_dir_val, masks_dir_val)\n",
    "copy_files(test_samples, images_dir, masks_dir, images_dir_test, masks_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we need to go from the original masks to ones that can be later transformed to labels that yolo is able to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/nachogris/Desktop/UNI/GCED/QUART/TAED2/LAB/TAED2_YOLOs/data/interim/transformed/images/test')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir_train = SPLIT_DATA_DIR / 'masks/train'\n",
    "output_dir_train = TRANSFORM_DATA_DIR / 'masks/train'\n",
    "input_dir_val = SPLIT_DATA_DIR / 'masks/val'\n",
    "output_dir_val = TRANSFORM_DATA_DIR / 'masks/val'\n",
    "input_dir_test = SPLIT_DATA_DIR / 'masks/test'\n",
    "output_dir_test = TRANSFORM_DATA_DIR / 'masks/test'\n",
    "\n",
    "output_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "from_raw_masks_to_image_masks(\n",
    "    input_dirs = [input_dir_train, input_dir_val, input_dir_test],\n",
    "    output_dirs = [output_dir_train, output_dir_val, output_dir_test]\n",
    ")\n",
    "\n",
    "images_dir_trans_train = TRANSFORM_DATA_DIR / 'images/train'\n",
    "images_dir_trans_val = TRANSFORM_DATA_DIR / 'images/val'\n",
    "images_dir_trans_test = TRANSFORM_DATA_DIR / 'images/test'\n",
    "\n",
    "shutil.copytree(images_dir_train, images_dir_trans_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_val, images_dir_trans_val, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_test, images_dir_trans_test, dirs_exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step we convert the previous transformed masks to some labels, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/nachogris/Desktop/UNI/GCED/QUART/TAED2/LAB/TAED2_YOLOs/data/processed/images/train')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir_train = TRANSFORM_DATA_DIR / 'masks/train'\n",
    "output_dir_train = LABELS_DATA_DIR / 'labels/train'\n",
    "input_dir_val = TRANSFORM_DATA_DIR / 'masks/val'\n",
    "output_dir_val = LABELS_DATA_DIR / 'labels/val'\n",
    "input_dir_test = TRANSFORM_DATA_DIR / 'masks/test'\n",
    "output_dir_test = LABELS_DATA_DIR / 'labels/test'\n",
    "\n",
    "output_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "output_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "from_image_masks_to_labels(\n",
    "    input_dirs = [input_dir_train, input_dir_val, input_dir_test],\n",
    "    output_dirs = [output_dir_train, output_dir_val, output_dir_test]\n",
    ")\n",
    "\n",
    "images_dir_labels_train = LABELS_DATA_DIR / 'images/train'\n",
    "images_dir_labels_val = LABELS_DATA_DIR / 'images/val'\n",
    "images_dir_labels_test = LABELS_DATA_DIR / 'images/test'\n",
    "\n",
    "shutil.copytree(images_dir_train, images_dir_labels_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_val, images_dir_labels_train, dirs_exist_ok = True)\n",
    "shutil.copytree(images_dir_test, images_dir_labels_train, dirs_exist_ok = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taed2-yolos-leWn8e5q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
