{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data pipeline has the objective to download and transform the original dataset to obtain our final dataset. Moreover, we will use DVC to keep data versioning control in every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Necessary imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import subprocess\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data_to_dvc(data_dir: str, commit_message: str | None = None) -> None:\n",
    "    commands = {\n",
    "        'dvc_add': f\"dvc add {data_dir}\",\n",
    "        'git_add': \"git add ../data.dvc\",\n",
    "        'git_commit': \"git commit -m \" + (commit_message if commit_message else \"Add data to DVC\"),\n",
    "        'dvc_push': \"dvc push\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add the data directory to DVC\n",
    "        subprocess.run(commands['dvc_add'].split(), check = True)\n",
    "\n",
    "        # Commit the changes\n",
    "        subprocess.run(commands['git_add'].split(), check = True)\n",
    "        subprocess.run(commands['git_commit'].split(), check = True)\n",
    "\n",
    "        # Push the changes to the remote repository\n",
    "        subprocess.run(commands['dvc_push'].split(), check = True)\n",
    "    except Exception as e:\n",
    "        print(\"Error is the following:\", e)\n",
    "        print(\"Something went wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(sample_list, src_images_dir, src_masks_dir, dest_images_dir, dest_masks_dir):\n",
    "    for sample in sample_list:\n",
    "        # Copy image file\n",
    "        src_image_path = os.path.join(src_images_dir, sample)\n",
    "        dest_image_path = os.path.join(dest_images_dir, sample)\n",
    "        shutil.copyfile(src_image_path, dest_image_path)\n",
    "\n",
    "        # Copy mask file (assuming the mask file has the same name as the image file)\n",
    "        sample_mask = sample.replace('jpg', 'png')\n",
    "\n",
    "        src_mask_path = os.path.join(src_masks_dir, sample_mask)\n",
    "        dest_mask_path = os.path.join(dest_masks_dir, sample_mask)\n",
    "        shutil.copyfile(src_mask_path, dest_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Varibale definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables are going to be defined based in a configuration file that will ease the process of variable modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML configuration file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Declare some configuration variables\n",
    "UPLOAD_ORIGINAL = config['dataPipelines']['dataDownloading']['uploadToDVC']\n",
    "DATASET_LINK = config['dataPipelines']['dataDownloading']['datasetLink']\n",
    "DATA_DIR = Path(config['dataPipelines']['dataDownloading']['dataDirectory'])\n",
    "COMMIT_MESSAGE_ORIGINAL = config['dataPipelines']['dataDownloading']['commitMessage']\n",
    "UPLOAD_SPLIT = config['dataPipelines']['splitData']['uploadToDVC']\n",
    "TRAIN_SIZE = config['dataPipelines']['splitData']['trainSize']\n",
    "VAL_SIZE = config['dataPipelines']['splitData']['valSize']\n",
    "TEST_SIZE = config['dataPipelines']['splitData']['testSize']\n",
    "SPLIT_DATA_DIR = Path(config['dataPipelines']['splitData']['dataDirectory'])\n",
    "COMMIT_MESSAGE_SPLIT = config['dataPipelines']['splitData']['commitMessage']\n",
    "\n",
    "# Create data directory if it does not exist\n",
    "DATA_DIR.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Load environment variables from a .env file and set up Kaggle credentials from environment variables\n",
    "load_dotenv()\n",
    "os.environ['KAGGLE_USERNAME'] = os.getenv('KAGGLE_USERNAME')\n",
    "os.environ['KAGGLE_KEY'] = os.getenv('KAGGLE_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download the original data. To do this we will use `KaggleAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mariarisques/dataset-person-yolos\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "api.dataset_download_files(DATASET_LINK, path = DATA_DIR, unzip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to upload the data to DVC to mantain our data versioning control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD_ORIGINAL:\n",
    "    upload_data_to_dvc(\n",
    "        data_dir = DATA_DIR,\n",
    "        commit_message = COMMIT_MESSAGE_ORIGINAL\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to split the data into train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "images_dir = DATA_DIR / 'dataset_person-yolos/data/images'\n",
    "masks_dir = DATA_DIR / 'dataset_person-yolos/data/masks'\n",
    "images_dir_train = SPLIT_DATA_DIR / 'train/images'\n",
    "masks_dir_train = SPLIT_DATA_DIR / 'train/masks'\n",
    "images_dir_val = SPLIT_DATA_DIR / 'val/images'\n",
    "masks_dir_val = SPLIT_DATA_DIR / 'val/masks'\n",
    "images_dir_test = SPLIT_DATA_DIR / 'test/images'\n",
    "masks_dir_test = SPLIT_DATA_DIR / 'test/masks'\n",
    "\n",
    "# Get the list of samples and shuffle them\n",
    "samples = os.listdir(images_dir)\n",
    "random.shuffle(samples)\n",
    "\n",
    "# Calculate split indices\n",
    "num_samples = len(samples)\n",
    "train_end = int(TRAIN_SIZE * num_samples)\n",
    "val_end = train_end + int(VAL_SIZE * num_samples)\n",
    "\n",
    "# Split samples\n",
    "train_samples = samples[:train_end]\n",
    "val_samples = samples[train_end:val_end]\n",
    "test_samples = samples[val_end:]\n",
    "\n",
    "# Create necessary directories\n",
    "SPLIT_DATA_DIR.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_train.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_val.mkdir(parents = True, exist_ok = True)\n",
    "images_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "masks_dir_test.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_samples, images_dir, masks_dir, images_dir_train, masks_dir_train)\n",
    "copy_files(val_samples, images_dir, masks_dir, images_dir_val, masks_dir_val)\n",
    "copy_files(test_samples, images_dir, masks_dir, images_dir_test, masks_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to upload the data to DVC to mantain our data versioning control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD_SPLIT:\n",
    "    upload_data_to_dvc(\n",
    "        data_dir = SPLIT_DATA_DIR,\n",
    "        commit_message = COMMIT_MESSAGE_SPLIT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taed2-yolos-leWn8e5q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
